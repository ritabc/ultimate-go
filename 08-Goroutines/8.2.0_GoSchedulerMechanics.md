# Go Scheduler Mechanics
* With every go program,
    - ![image](https://user-images.githubusercontent.com/11031915/68126210-44fc6880-fee1-11e9-83b2-7c25d6d7b022.png)
    - Processor P is given an Machine M, representing an OS thread or OS path of execution
    - M is responsible for scheduling
    - Also: 2 RQ's
        1. Global Run Queue exists, GRQ
        1. Every P has Local Run Queue, LRQ
    - Every goroutine can also be in Running, Runnable, or Waiting state
    - Say Main Goroutine, Gm is executing on P and M
    - Gm creates a few more goroutines G
    - These G's are in runnable state
        * usually they will be in P's LRQ
        * but occasionally they will be in GRQ b/c a P hasn't taken it yet (this is a work-stealing run queue)
* Go scheduler runs in user mode, or user space. 
    - vs kernel mode, in which P can do whatever it wants
    - user mode puts P in protected mode
    - This means go scheduler is not preemptive, non-deterministic
    - The go scheduler is instead cooperative
    - Go scheduler is doing cooperation, not go developers, so to users, it feels non-deterministic, preemptive
* For runtime scheduler to make decision, events must happen - context switching happens at function call level (for now)
    - 3 classes of events that allow scheduler to make context switching decision
        1. `go` keyword for goroutines
        1. garbage collection
        1. sys calls (log, fmt.Print, etc)
        - also, blocking calls 
* Sys calls: we're fortunate that prod system OS have ability to do asynchronous system calls
    * we use this for networking, disk i/o, etc
    * We try to leverage this as much as possible
    * Say we're on single threaded (1 P) program, and we make system call: open file or make network request that could take seconds. This is STOP THE WORLD situation == bad
    * Instead, go uses network poller
        - When goroutine wants to make system call, the gorouting is switched off P and onto Network Poller, it makes it request, is placed in Waiting state, and main P's M is freed up for next Runnable goroutine
        - When network goroutine request comes back, its put back into Runnable state into main P's LRQ
    * This allows us to maintain small # of threads - ideally 1 per core?
* What if we're on OS that doesn't support async calls || we have blocking call that can't be handled async || working with cgo
    - Goroutine wants to make a call, blocks M1
    - Scheduler will detach M1 and Gm
    - Scheduler will bring new thread M2, which can handle new runnable goroutine
    - program can technically handle 10K detached bundles, but this is not normal
* Work Stealing Scheduler
    - in multi-P environment
    - say one P has no more work
    - it will first look in GRQ for runnable goroutines
    - Then it will look in another P's LRQ
    - `Spinning`: when M doesn't have work to do and is look for other goroutines
        * We want to minimize spinning time
        * Mechanical sympathy comes in: we want to keep these threads busy
* Imagine writing traditional, multithreaded software, using real OS threads on multicore machine
    * Say 2 threads need to pass msg's back and forth
    * T1 sends msg to T2, T1 Context Switches (CS's) to Waiting State
    * T2 receives msg, Waiting -> Runnable -> Executing then sends msg to T1
        - T2 enters Watiing
    * Rinse, Repeat every time orchestratation (2 threads talking to each other) happens
    * CS is very expensive every time threads coming on & off Cores
* Imagine, however, same problem with single P - single threaded, with goroutines
    * We still need CS for goroutines to talk to each other
    * go Scheduler knows what goroutine is doing, so less state transfer information
    * Same i/o happening with goroutines
    * But during every CS in go, from OS perspective, that thread NEVER went into waiting state - always in Running OR Executing state
    * Takeaway: Go turned I/O bound work into CP bound work: 
        - when working with CP bound work, more threads than cores can only add load
        * load off OS is minimized

