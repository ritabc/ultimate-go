# OS Scheduler Mechanics
## Misc
* OS Scheduler is a preemptive scheduler: When the scheduler is making a decision about how to schedule threads, we CANNOT predict the outcome: its non-deterministic
* When OS starts, a process is started
    - process is container for resources the process will need (2 types)
        1. Key Resource: Memory
            - process has been given memory map
        2. Key Resource: Thread
            - process has been given the main thread
            - when main thread dies, process is shut down
            - a thread is a path of execution, in a linear fashion
* Scheduler doesn't care about processes, only threads or paths of execution
* Thread will be in 1 of 3 states
    1. Running or executing: has been placed on core, and whatever is next instruction (where it's pointing to). Will continue executing its instructions until OS (in nondeterministic way) determines that thread is no longer able to execute on that core. At that point, a context switch will happen
        - Context Switch: When a thread, executing on a core, gets pulled off core and another thread gets placed there. 
            - Context switches are expensive
            - OS doesn't know what threads are doing
            - OS knows threads are runnable or running, but not what threads are doing
            - Much info must be saved out of core, so when thread is taken off core and then later placed back on, it can resume without being able to tell it took a break
    1. Runnable: OS will choose all runnable threads and can place it on the core during context switch 
    1. Waiting
        - has many unimportant substates
        - from execution standpoint, it's like it's in the matrix
            - disappears from view until it goes back into runnable state
        - waiting for something on OS, from io, from network, etc. 
* In early Days:
    - Only 1 core on a hardware
    - 1 core can only handle 1 path of execution at a time
    - What if you have 10k paths of execution that want access to the core?
    - OS's job is to make it seem like all 10k are operating at same time
    - OS scheduler must give each of 10k threads a slice of time on the core
* Less is More : fewer threads are better
    - more threads mean more expensive context switches
    - When context switching, Core is not handling threads - it's handling OS code
* In 2004, multiCore systems became norm
    - 2 threads on 2 cores can be run in parallel
    - before, concurrency (managment of multiple threads at once, on 1 core), was a thing, but now multicore systems introduced parallelism
    - With this, academics researched that OS scheduler didn't know how to officially handle this situation
        - runnable threads existed AT The same time as Cores not running anyhting for nano-seconds to microseconds
    - So, Schedulers needed to be re-written
* Scheduler is Very complex re: Efficiency
    - Having multiple cores talk to each other, but some are closer together than others and it's easier for them to talk
    * Say, 4 core machine  
        - have T1 (main thread) (associated with C1)
        - T1 decides to start another thread: T2 (associated with C2)
        - We're on a busy machine, which has caching system
        - Should the scheduler, which knows C1 and C2 are busy, pull T1 and put T2 on C1 (because T1 and T2 are related and share data)
        - Many other complex decisions scheduler makes
* Scheduler makes Run Queues
    - RQ's can exist at Core level, processor level
    - RQ's help scheduler make decisions efficiently
## Mechanical Sympathies of Go Scheduler
* Given this, we have 2 responsibilities
    1. Understand workload: Understand synchronization and orchestration
        - CP-bound workload: Threads will never move from running -> waiting (they'll never be asking for anything that will require waitin)
            - if we have this work, having more threads than cores does us no good b/c context switching is not good
        - IO bound workload: 
            - with this work, having more threads per core is good
            - as thread moves from running to waiting, threads become available
    1. How does Go Scheduler (which sits on top of OS Sched) works
* Historically, we used thread pools
    - IOCompletionPort: IOCP, windows solution to multithreading
    - as requests came in over the wire, they would be posting into thread pool, then let family of threads in pool work on them
    - But, how do you know how to size threads? 
    - Also, pool needs to be configured for hardware
    - Ideally, as # of cores increases, throughput should increase linearly
    - If its not linear growth, we have a mechanical sympathy problem
    - IOCP has:
        - min 2
        - max 24
        - concurrency 0: If only 1 thread on Core, only allow 1 

